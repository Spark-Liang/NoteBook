#### 数据源

- [通用语法](#通用语法)
  
  - 通用选项

- [csv](#csv)

- [jdbc](#JDBC)

- 参考文档

##### 通用语法

有两种常用的通过数据源创建DF 的语法：

```scala
// 方式1

spark.read.format("csv") // 所需的文件格式
    .option("<option key>","<option value>")
    .load("<file path>") 

// 方式2

spark.read.csv // spark 内置了部分常用的格式
    .option("<option key>","<option value>")
    .load("<file path>") 
```

**其中\<file path\>，允许使用通配符\*，比如: data/\*/\*.dat.**

###### 通用选项

| 选项名                 | 解释                             | 样例           |
| ------------------- | ------------------------------ | ------------ |
| pathGlobFilter      | 文件名的筛选条件。只筛选符合条件的文件名，其中“*”是通配符 | *.parquet    |
| recursiveFileLookup | 是否递归                           | true 或 false |



##### csv

所有可选选项：

| 参数                          | 解释                                                                                                                                                                                                                                         |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `sep`                       | 默认是`,` 指定单个字符分割字段和值                                                                                                                                                                                                                        |
| `encoding`                  | 默认是`uft-8`通过给定的编码类型进行解码                                                                                                                                                                                                                    |
| `quote`                     | 默认是`“`，其中分隔符可以是值的一部分，设置用于转义带引号的值的单个字符。如果您想关闭引号，则需要设置一个空字符串，而不是`null`。                                                                                                                                                                      |
| `escape`                    | 默认(`\`)设置单个字符用于在引号里面转义引号                                                                                                                                                                                                                   |
| `charToEscapeQuoteEscaping` | 默认是转义字符（上面的`escape`）或者`\0`，当转义字符和引号(`quote`)字符不同的时候，默认是转义字符(escape)，否则为`\0`                                                                                                                                                                |
| `comment`                   | 默认是空值，设置用于跳过行的单个字符，以该字符开头。默认情况下，它是禁用的                                                                                                                                                                                                      |
| `header`                    | 默认是`false`，将第一行作为列名                                                                                                                                                                                                                        |
| `enforceSchema`             | 默认是`true`， 如果将其设置为`true`，则指定或推断的模式将强制应用于数据源文件，而`CSV`文件中的标头将被忽略。 如果选项设置为`false`，则在`header`选项设置为`true`的情况下，将针对CSV文件中的所有标题验证模式。模式中的字段名称和CSV标头中的列名称是根据它们的位置检查的，并考虑了*`spark.sql.caseSensitive`。虽然默认值为`true`，但是建议禁用 `enforceSchema`选项，以避免产生错误的结果 |
| `inferSchema`               | inferSchema`（默认为`false`）：从数据自动推断输入模式。 *需要对数据进行一次额外的传递                                                                                                                                                                                      |
| `samplingRatio`             | 默认为`1.0`,定义用于模式推断的行的分数                                                                                                                                                                                                                     |
| `ignoreLeadingWhiteSpace`   | 默认为`false`,一个标志，指示是否应跳过正在读取的值中的前导空格                                                                                                                                                                                                        |
| `ignoreTrailingWhiteSpace`  | 默认为`false`一个标志，指示是否应跳过正在读取的值的结尾空格                                                                                                                                                                                                          |
| `nullValue`                 | 默认是空的字符串,设置null值的字符串表示形式。从2.0.1开始，这适用于所有支持的类型，包括字符串类型                                                                                                                                                                                      |
| `emptyValue`                | 默认是空字符串,设置一个空值的字符串表示形式                                                                                                                                                                                                                     |
| `nanValue`                  | 默认是`Nan`,设置非数字的字符串表示形式                                                                                                                                                                                                                     |
| `positiveInf`               | 默认是`Inf`                                                                                                                                                                                                                                   |
| `negativeInf`               | 默认是`-Inf` 设置负无穷值的字符串表示形式                                                                                                                                                                                                                   |
| `dateFormat`                | 默认是`yyyy-MM-dd`,设置指示日期格式的字符串。自定义日期格式遵循`java.text.SimpleDateFormat`中的格式。这适用于日期类型                                                                                                                                                            |
| `timestampFormat`           | 默认是`yyyy-MM-dd'T'HH:mm:ss.SSSXXX`，设置表示时间戳格式的字符串。自定义日期格式遵循`java.text.SimpleDateFormat`中的格式。这适用于时间戳记类型                                                                                                                                       |
| `maxColumns`                | 默认是`20480`定义多少列数目的硬性设置                                                                                                                                                                                                                     |
| `maxCharsPerColumn`         | 默认是`-1`定义读取的任何给定值允许的最大字符数。默认情况下为-1，表示长度不受限制                                                                                                                                                                                                |
| `mode`                      | 默认（允许）允许一种在解析过程中处理损坏记录的模式。它支持以下不区分大小写的模式。请注意，`Spark`尝试在列修剪下仅解析`CSV`中必需的列。因此，损坏的记录可以根据所需的字段集而有所不同。可以通过`spark.sql.csv.parser.columnPruning.enabled`（默认启用）来控制此行为。                                                                             |
| mode下面的参数:                  | ---------------------------------------------------                                                                                                                                                                                        |
| `PERMISSIVE`                | 当它遇到损坏的记录时，将格式错误的字符串放入由“ columnNameOfCorruptRecord”配置的*字段中，并将其他字段设置为“ null”。为了保留损坏的记录，用户可以在用户定义的模式中设置一个名为`columnNameOfCorruptRecord`                                                                                                       |
| `DROPMALFORMED`             | 忽略整个损坏的记录                                                                                                                                                                                                                                  |
| `FAILFAST`                  | 遇到损坏的记录时引发异常                                                                                                                                                                                                                               |
| mode参数结束                    | -------------------------------------------------------                                                                                                                                                                                    |
| `columnNameOfCorruptRecord` | 默认值指定在`spark.sql.columnNameOfCorruptRecord`,允许重命名由`PERMISSIVE`模式创建的格式错误的新字段。这会覆盖`spark.sql.columnNameOfCorruptRecord`                                                                                                                      |
| `multiLine`                 | 默认是`false`,解析一条记录，该记录可能跨越多行                                                                                                                                                                                                                |



##### JDBC

[JDBC To Other Databases - Spark 3.0.1 Documentation](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)



##### 参考文档

- [Generic File Source Options - Spark 3.0.1 Documentation](https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#ignore-missing-files)

- [Spark 读取csv文件操作，option参数解释_OldDirverHelpMe的博客-CSDN博客](https://blog.csdn.net/OldDirverHelpMe/article/details/106120312)


